{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNriuLxgKpgMu2V2KRm/x83",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Sjleerodls/Data_Analysis/blob/main/lab_da/ml21_llm.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# LLM (Large Language Model, 거대 언어 모델)"
      ],
      "metadata": {
        "id": "bWFrjJ2h6Kjg"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* 시퀀스-시퀀스 작업(Sequence-to-Sequence)\n",
        "    * 시퀀스 데이터를 입력받아서 시퀀스 데이터를 출력하는 작업\n",
        "    * 시퀀스 처리(NLP, Natural Language Processing) 분야에서 요약, 번역 등의 작업 해당\n",
        "    * 두 개의 (순환) 신경망을 연결한 인코더-디코더(encoder-decoder) 구조가 널리 사용\n",
        "* 어텐션 메커니즘(Attention mechanism)\n",
        "    * 인코더-디코더 구조에서 사용된 순환 신경망의 성능을 향상시키기 위해서 고안된 알고리즘이 어텐션 메커니즘\n",
        "    * 기존에는 인코더의 마지막 타임스텝에서 출력한 은닉 상태만을 사용해서 디코더가 새로운 텍스트를 생성\n",
        "    * 어텐션 메커니즘은 모든 타임 스텝에서 인코더가 출력한 은닉 상태를 디코더가 참조할 수 있도록 고안\n",
        "    * 디코더가 새로운 토큰을 생성할 때 인코더가 처리한 토큰들 중에서 어떤 토큰에 주의(attention)를 더 기울여야 할지 결정. (입력 토큰들마다 디코더가 중요도를 다르게 부여)\n",
        "* 트랜스포커 모델(Transformer model)\n",
        "    * 어텐션 메커니즘을 기반으로 하여 인코더-디코더 구조에서 순환층을 제거\n",
        "    * 인코더에서 한 번에 하나의 토크씩 처리하지 않고 입력 테스트 전체를 한 번에 처리\n",
        "    * 핵심 구성 요소\n",
        "        * 멀티 헤드 어텐션 (multi-head attention)\n",
        "        * 층 정규화 (layer normalization)\n",
        "        * 잔차 연결 (residual connection)\n",
        "        * 피드 포워드 네트워크 (feed-forward network)"
      ],
      "metadata": {
        "id": "BvTojw1M6Usx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "<img src=\"https://pbs.twimg.com/media/GJg3QtMXwAAvUT4?format=jpg&name=large\"\n",
        "    alt=\"LLM 가계도\" />"
      ],
      "metadata": {
        "id": "WPax7tPfAlEN"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Encoder 기반 모델\n",
        "    * 텍스트 (긍정/부정) 분류\n",
        "    * 개체명 인식 - 텍스트에서 사람 이름, 지역 이름, 회사 이름 등의 고유 명사를 식별.\n",
        "    * BERT, RoBERTa, ...\n",
        "* Encoder-Decoder 기반 모델\n",
        "    * 문서 요약, 번역, 질문-답변(질문과 문맥 텍스트가 주어졌을 때 문맥 안에서 답을 찾아서 생성)\n",
        "    * T5, BART, ...\n",
        "* Decoder 기반 모델\n",
        "    * 텍스트 생성 - 챗봇, 질문 답변, 요약, 번역\n",
        "    * 디코더 :\n",
        "        * 이전까지 생성한 텍스트를 입력받아 다음 토큰을 예측하는 방식\n",
        "        * 인코더로부터의 입력이 없으면 디코더는 아무것도 생성할 수 없음.\n",
        "        * 이전에 생성한 텍스트 인 것처럼 어떤 텍스트를 입력해 주면 인코더 도움 없이 다음 토큰을 예측.\n",
        "        * 프롬프트(prompt) : 이전에 생성한 텍스트인 것처럼 전달하는 초기 텍스트.\n",
        "    * GPT-4, GPT-5, LLaMA, ...\n",
        "    * 현재 가장 활발히 연구되고 있는 LLM 분야."
      ],
      "metadata": {
        "id": "cDLNOHMxGwMz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MxdnXb6T6Hzl"
      },
      "outputs": [],
      "source": []
    }
  ]
}